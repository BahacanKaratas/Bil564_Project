{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5Yzwy621gDC"
      },
      "source": [
        "# BIL564 Derin √ñƒürenme Projesi ‚Äì GAN ile Veri Artƒ±rƒ±mƒ±\n",
        "\n",
        "## Proje Hedefi:\n",
        "Bu notebook'ta GAN (Generative Adversarial Network) mimarisi kullanƒ±larak, IQ-OTH/NCCD akciƒüer kanseri g√∂r√ºnt√º veri seti √ºzerinden **sentetik g√∂r√ºnt√º √ºretimi** ger√ßekle≈ütirilmi≈ütir. GAN ile √ºretilen bu g√∂r√ºnt√ºler, azƒ±nlƒ±k sƒ±nƒ±flarƒ±n temsilini g√º√ßlendirmek ve CNN modelinin sƒ±nƒ±flandƒ±rma performansƒ±nƒ± artƒ±rmak amacƒ±yla kullanƒ±lmaktadƒ±r.\n",
        "\n",
        "## Uygulanan Y√∂ntemler:\n",
        "- **GAN Eƒüitimi**: Her sƒ±nƒ±f (normal, benign, malignant) i√ßin ayrƒ± GAN mimarileri olu≈üturulmu≈ütur.\n",
        "- **Generator & Discriminator**: Giri≈ü olarak rastgele latent vekt√∂r alƒ±p sahte g√∂r√ºnt√ºler √ºretir; discriminator ise bu g√∂r√ºnt√ºlerin ger√ßekliƒüini deƒüerlendirir.\n",
        "- **Discriminator Tabanlƒ± Filtreleme**: Yalnƒ±zca `g√ºven skoru > 0.95` olan sentetik g√∂r√ºnt√ºler eƒüitimde kullanƒ±lmak √ºzere se√ßilmi≈ütir.\n",
        "- **G√∂rselle≈ütirme**: Ger√ßek ve sentetik g√∂r√ºnt√ºler g√∂rsel olarak kar≈üƒ±la≈ütƒ±rƒ±lmƒ±≈ü, kalite deƒüerlendirmesi yapƒ±lmƒ±≈ütƒ±r.\n",
        "\n",
        "## Ana A≈üamalar:\n",
        "1. Kaggle √ºzerinden veri setinin indirilmesi\n",
        "2. G√∂r√ºnt√ºlerin yeniden boyutlandƒ±rƒ±lmasƒ± ve TensorFlow dataset'e d√∂n√º≈üt√ºr√ºlmesi\n",
        "3. GAN mimarilerinin tanƒ±mlanmasƒ± ve kayƒ±p fonksiyonlarƒ±nƒ±n olu≈üturulmasƒ±\n",
        "4. Her sƒ±nƒ±f i√ßin GAN eƒüitimi ve sentetik veri √ºretimi\n",
        "5. Y√ºksek kaliteli sahte g√∂r√ºnt√ºlerin `.png` formatƒ±nda kaydedilmesi\n",
        "6. Ger√ßek ve GAN g√∂r√ºnt√ºlerinin g√∂rselle≈ütirilmesi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilIB6U-L-Fwy"
      },
      "source": [
        "## Kaggle √úzerinden Veri Setinin ƒ∞ndirilmesi\n",
        "\n",
        "Bu h√ºcrede Kaggle API kullanƒ±larak IQ-OTHNCCD akciƒüer kanseri veri seti indirilmektedir.  \n",
        "- `kaggle.json` dosyasƒ± y√ºklenerek API eri≈üimi saƒülanƒ±r.  \n",
        "- Dataset, √ßalƒ±≈üma dizinine `.zip` formatƒ±nda indirilir.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "id": "K5SqSMDB3kW7",
        "outputId": "8d6f260f-d47e-4030-ccae-6c8f84013167"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import os, zipfile\n",
        "\n",
        "# 1. kaggle.json dosyasƒ±nƒ± y√ºkle\n",
        "print(\"L√ºtfen kaggle.json dosyanƒ± y√ºkle:\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# 2. Kaggle API ayarlarƒ±\n",
        "os.makedirs(\"/root/.kaggle\", exist_ok=True)\n",
        "with open(\"/root/.kaggle/kaggle.json\", \"wb\") as f:\n",
        "    f.write(uploaded[\"kaggle.json\"])\n",
        "os.chmod(\"/root/.kaggle/kaggle.json\", 600)\n",
        "\n",
        "# 3. Dataset'i indir (senin verdiƒüin path)\n",
        "!kaggle datasets download -d hamdallak/the-iqothnccd-lung-cancer-dataset\n",
        "\n",
        "# 4. Zip dosyasƒ±nƒ± √ßƒ±kar\n",
        "with zipfile.ZipFile(\"the-iqothnccd-lung-cancer-dataset.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"lung_dataset\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYT6fcWu-nMS"
      },
      "source": [
        "## Kaggle'dan ƒ∞ndirilen ZIP Dosyasƒ±nƒ±n A√ßƒ±lmasƒ±\n",
        "\n",
        "Bu h√ºcrede, Kaggle √ºzerinden indirilen `The IQ-OTHNCCD lung cancer dataset.zip` dosyasƒ± √ßƒ±karƒ±lmaktadƒ±r.  \n",
        "- ZIP dosyasƒ±nƒ±n yolu manuel olarak belirtilmi≈ütir.  \n",
        "- ƒ∞√ßerikler `/content/lung_dataset` klas√∂r√ºne a√ßƒ±lƒ±r.  \n",
        "\n",
        "> Bu adƒ±m, verileri eƒüitimde kullanmak √ºzere hazƒ±r hale getirir.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weVaA5yB_5Ey"
      },
      "source": [
        "## Gerekli K√ºt√ºphanelerin Y√ºklenmesi\n",
        "\n",
        "Bu h√ºcrede proje boyunca kullanƒ±lacak t√ºm k√ºt√ºphaneler i√ße aktarƒ±lmaktadƒ±r.  \n",
        "- `TensorFlow` ve `Keras`: GAN, CNN modeli ve veri i≈üleme i≈ülemleri i√ßin kullanƒ±lƒ±r.  \n",
        "- `NumPy`, `Matplotlib`, `PIL`, `os`: G√∂r√ºnt√º i≈üleme, dosya y√∂netimi ve g√∂rselle≈ütirme i√ßin kullanƒ±lƒ±r.  \n",
        "- `scikit-learn`: Veri setini eƒüitim/test olarak ayƒ±rmak i√ßin kullanƒ±lƒ±r.  \n",
        "- Kod tekrarƒ± ve karma≈üayƒ± √∂nlemek amacƒ±yla gereksiz ve yinelenen importlar temizlenmi≈ütir.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1v3Dyh2n1gDF",
        "outputId": "ecb5548e-af4b-4cc0-a450-3cbfc6c7f4dc"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.utils import image_dataset_from_directory\n",
        "import os\n",
        "import imghdr\n",
        "import random\n",
        "from pathlib import Path\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import image_dataset_from_directory\n",
        "from tensorflow.keras.layers import Rescaling\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import numpy as np\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import random\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import numpy as np\n",
        "import os\n",
        "from PIL import Image\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, GlobalAveragePooling2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import load_model\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7yZtuT8bGqB"
      },
      "source": [
        "## Veri Setinin Y√ºklenmesi ve √ñn ƒ∞≈üleme\n",
        "\n",
        "Bu h√ºcrede IQ-OTHNCCD akciƒüer kanseri veri seti TensorFlow `image_dataset_from_directory` fonksiyonu ile y√ºklenmektedir.\n",
        "\n",
        "### Temel Adƒ±mlar:\n",
        "- **Yol Tanƒ±mlama:** G√∂rsellerin bulunduƒüu dizin belirtilmi≈ütir.\n",
        "- **Parametreler:** G√∂rseller 128x128 boyutuna yeniden boyutlandƒ±rƒ±lmƒ±≈ü, batch boyutu 32 olarak ayarlanmƒ±≈ütƒ±r.\n",
        "- **Normalization:** G√∂rseller [-1, 1] aralƒ±ƒüƒ±na normalize edilmi≈ütir (`Rescaling(1./127.5, offset=-1)`).\n",
        "- **Train/Test Ayrƒ±mƒ±:** %80 eƒüitim, %20 doƒürulama olarak ayrƒ±lmƒ±≈ütƒ±r.\n",
        "\n",
        "### Performans Optimizasyonu:\n",
        "- `cache()`, `prefetch()` ve `shuffle()` i≈ülemleri eƒüitim s√ºrecini hƒ±zlandƒ±rmak i√ßin kullanƒ±lmƒ±≈ütƒ±r.\n",
        "\n",
        "### Sƒ±nƒ±f Bilgisi:\n",
        "- Veri setinde 3 sƒ±nƒ±f mevcuttur: **Benign**, **Malignant** ve **Normal**.\n",
        "- Bu sƒ±nƒ±flar etiket-dizin e≈üle≈ümeleri ile bir s√∂zl√ºkte tutulmaktadƒ±r.\n",
        "\n",
        "### Sƒ±nƒ±f Bazlƒ± Ayrƒ±m:\n",
        "- Eƒüitim verisi i√ßerisinden her bir sƒ±nƒ±f (benign, malignant, normal) i√ßin ayrƒ± dataset‚Äôler filtrelenmi≈ütir.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_KnwT2nbGGl",
        "outputId": "5ec643bc-0a00-4135-9284-7086656167ee"
      },
      "outputs": [],
      "source": [
        "dataset_path = Path(\"/content/lung_dataset/The IQ-OTHNCCD lung cancer dataset\")\n",
        "\n",
        "image_size = (128, 128)\n",
        "batch_size = 32\n",
        "seed = 123\n",
        "\n",
        "normalization_layer = Rescaling(1./127.5, offset=-1)\n",
        "def preprocess_image(image, label):\n",
        "    image = tf.cast(image, tf.float32)\n",
        "    image = normalization_layer(image)\n",
        "    return image, label\n",
        "\n",
        "raw_train_dataset = image_dataset_from_directory(\n",
        "    dataset_path,\n",
        "    validation_split=0.2,\n",
        "    subset=\"training\",\n",
        "    seed=seed,\n",
        "    image_size=image_size,\n",
        "    batch_size=batch_size\n",
        ")\n",
        "raw_val_dataset = image_dataset_from_directory(\n",
        "    dataset_path,\n",
        "    validation_split=0.2,\n",
        "    subset=\"validation\",\n",
        "    seed=seed,\n",
        "    image_size=image_size,\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "class_names = raw_train_dataset.class_names\n",
        "print(\"Sƒ±nƒ±f ƒ∞simleri:\", class_names)\n",
        "\n",
        "train_dataset = (\n",
        "    raw_train_dataset\n",
        "    .map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    .shuffle(1000)\n",
        "    .cache()\n",
        "    .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        ")\n",
        "\n",
        "val_dataset = (\n",
        "    raw_val_dataset\n",
        "    .map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    .cache()\n",
        "    .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        ")\n",
        "\n",
        "label_to_index = {name: i for i, name in enumerate(class_names)}\n",
        "print(\"Etiket e≈üle≈ümeleri:\", label_to_index)\n",
        "\n",
        "\n",
        "def filter_by_class(dataset, class_index):\n",
        "    return dataset.filter(lambda img, label: tf.reduce_any(tf.equal(label, class_index)))\n",
        "\n",
        "train_dataset_benign = filter_by_class(train_dataset, label_to_index[\"Bengin cases\"])\n",
        "train_dataset_malignant = filter_by_class(train_dataset, label_to_index[\"Malignant cases\"])\n",
        "train_dataset_normal = filter_by_class(train_dataset, label_to_index[\"Normal cases\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XnkJ_B71gDG"
      },
      "source": [
        "## GAN Mimarisi: Generator ve Discriminator\n",
        "\n",
        "### Generator\n",
        "- Giri≈ü: 100 boyutlu rastgele bir latent vekt√∂r.\n",
        "- 4 katmanlƒ± transpoz konvol√ºsyonel yapƒ± ile 128x128 boyutunda RGB g√∂r√ºnt√ºler √ºretir.\n",
        "- Aktivasyon: `tanh` ile [-1, 1] aralƒ±ƒüƒ±nda normalize edilmi≈ü √ßƒ±ktƒ± verir.\n",
        "\n",
        "### Discriminator\n",
        "- Giri≈ü: 128x128 boyutunda bir g√∂r√ºnt√º (ger√ßek veya sahte).\n",
        "- 4 katmanlƒ± konvol√ºsyonel yapƒ± ve sonrasƒ±nda `Dense(1)` ile ger√ßeklik olasƒ±lƒ±ƒüƒ± tahmini yapar.\n",
        "- Aktivasyon: `sigmoid`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ijCOHeap1gDG",
        "outputId": "71d5ea97-eaaf-4bbe-cedf-b9b20a397b6a"
      },
      "outputs": [],
      "source": [
        "def build_generator():\n",
        "    model = tf.keras.Sequential([\n",
        "        layers.Dense(16 * 16 * 256, use_bias=False, input_shape=(100,)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Reshape((16, 16, 256)),\n",
        "\n",
        "        layers.Conv2DTranspose(128, (3, 3), strides=(2, 2), padding='same', use_bias=False),  # (16->32)\n",
        "        layers.BatchNormalization(),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "\n",
        "        layers.Conv2DTranspose(64, (3, 3), strides=(2, 2), padding='same', use_bias=False),   # (32->64)\n",
        "        layers.BatchNormalization(),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "\n",
        "        layers.Conv2DTranspose(3, (3, 3), strides=(2, 2), padding='same', activation='tanh')  # (64->128)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "def build_discriminator():\n",
        "    model = tf.keras.Sequential([\n",
        "        # Input layer accepts 128x128 RGB images\n",
        "        layers.InputLayer(input_shape=(128, 128, 3)),\n",
        "\n",
        "        # First convolution: 128x128 -> 64x64\n",
        "        layers.Conv2D(64, (3, 3), strides=(2, 2), padding='same'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Dropout(0.4),\n",
        "\n",
        "        # Second convolution: 64x64 -> 32x32\n",
        "        layers.Conv2D(128, (3, 3), strides=(2, 2), padding='same'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Dropout(0.4),\n",
        "\n",
        "        # Third convolution: 32x32 -> 16x16\n",
        "        layers.Conv2D(256, (3, 3), strides=(2, 2), padding='same'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Dropout(0.4),\n",
        "\n",
        "        # Fourth convolution: 16x16 -> 8x8\n",
        "        layers.Conv2D(512, (3, 3), strides=(2, 2), padding='same'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Dropout(0.5),\n",
        "\n",
        "        # Flatten and output a single probability\n",
        "        layers.Flatten(),  # Output shape: (None, 8*8*512) = (None, 32768)\n",
        "        layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Normal GAN\n",
        "gen_normal = build_generator()\n",
        "disc_normal = build_discriminator()\n",
        "\n",
        "# Benign GAN\n",
        "gen_benign = build_generator()\n",
        "disc_benign = build_discriminator()\n",
        "\n",
        "# Malignant GAN\n",
        "gen_malignant = build_generator()\n",
        "disc_malignant = build_discriminator()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3e7TAJ0BXIV"
      },
      "source": [
        "## GAN Kayƒ±p Fonksiyonlarƒ± (Loss Functions)\n",
        "\n",
        "### Discriminator Loss\n",
        "- Ger√ßek g√∂r√ºnt√ºler i√ßin `label=1`, sahte g√∂r√ºnt√ºler i√ßin `label=0` hedeflenir.\n",
        "- Ger√ßek ve sahte g√∂r√ºnt√ºler i√ßin ayrƒ± ayrƒ± `BinaryCrossentropy` hesaplanƒ±r.\n",
        "- Toplam kayƒ±p: `real_loss + fake_loss`.\n",
        "\n",
        "### Generator Loss\n",
        "- Ama√ß: √úretilen sahte g√∂r√ºnt√ºlerin `Discriminator` tarafƒ±ndan **ger√ßek** olarak sƒ±nƒ±flandƒ±rƒ±lmasƒ±nƒ± saƒülamaktƒ±r.\n",
        "- Bu nedenle, t√ºm `fake_output`‚Äôlar i√ßin `label=1` olarak verilir.\n",
        "\n",
        "> `BinaryCrossentropy(from_logits=False)` kullanƒ±lmƒ±≈ütƒ±r √ß√ºnk√º √ßƒ±ktƒ± aktivasyon fonksiyonu `sigmoid` ile normalize edilmi≈ütir.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d5KV8LNK1gDG"
      },
      "outputs": [],
      "source": [
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
        "\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "    real_output = tf.cast(real_output, tf.float32)\n",
        "    fake_output = tf.cast(fake_output, tf.float32)\n",
        "\n",
        "    real_labels = tf.ones_like(real_output, dtype=tf.float32)\n",
        "    fake_labels = tf.zeros_like(fake_output, dtype=tf.float32)\n",
        "\n",
        "    real_loss = cross_entropy(real_labels, real_output)\n",
        "    fake_loss = cross_entropy(fake_labels, fake_output)\n",
        "    return real_loss + fake_loss\n",
        "\n",
        "def generator_loss(fake_output):\n",
        "    fake_output = tf.cast(fake_output, tf.float32)\n",
        "    labels = tf.ones_like(fake_output, dtype=tf.float32)\n",
        "    return cross_entropy(labels, fake_output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1Te_3pa1gDG"
      },
      "source": [
        "### Optimizers\n",
        "\n",
        "- Bu h√ºcrede, hem Generator hem de Discriminator i√ßin Adam optimizasyon algoritmasƒ± tanƒ±mlanmƒ±≈ütƒ±r. Bu optimizasyon, aƒülarƒ±n √∂ƒürenme s√ºrecini hƒ±zlandƒ±rmak ve stabil hale getirmek i√ßin kullanƒ±lƒ±r.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GovvWL361gDG"
      },
      "outputs": [],
      "source": [
        "generator_optimizer = tf.keras.optimizers.AdamW(learning_rate=0.0001, weight_decay=1e-4, beta_1=0.5, beta_2=0.999)\n",
        "discriminator_optimizer = tf.keras.optimizers.AdamW(learning_rate=0.0002, weight_decay=1e-4, beta_1=0.5, beta_2=0.999)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8lNmPzUBxvF"
      },
      "source": [
        "## GAN Eƒüitim S√ºreci\n",
        "\n",
        "### `train_step()`:\n",
        "- Bir batch veri alƒ±narak:\n",
        "  - Latent vekt√∂r ile sahte g√∂r√ºnt√ºler √ºretilir.\n",
        "  - Ger√ßek ve sahte g√∂r√ºnt√ºler `Discriminator` √ºzerinden ge√ßer.\n",
        "  - Her iki modelin loss deƒüerleri hesaplanƒ±r ve geri yayƒ±lƒ±m (backpropagation) yapƒ±lƒ±r.\n",
        "\n",
        "---\n",
        "\n",
        "### `train_gan()`:\n",
        "- Verilen epoch sayƒ±sƒ± kadar `train_step()` √ßalƒ±≈ütƒ±rƒ±lƒ±r.\n",
        "- Her `vis_interval` epoch‚Äôta bir sentetik g√∂r√ºnt√ºler olu≈üturularak g√∂rselle≈ütirilir.\n",
        "- Eƒüitim sonunda:\n",
        "  - Generator ve Discriminator modelleri `.h5` formatƒ±nda kaydedilir.\n",
        "  - Final sentetik √∂rnekler g√∂sterilir.\n",
        "\n",
        "---\n",
        "\n",
        "### `generate_and_plot_images()`:\n",
        "- Generator ile √∂rnek latent vekt√∂rlerden g√∂rseller √ºretilir.\n",
        "- G√∂rseller bir grid ≈üeklinde g√∂rselle≈ütirilir (`matplotlib` ile).\n",
        "\n",
        "> Bu yapƒ±, ileride entegre edeceƒüimiz **Streamlit uygulamasƒ±nda** da kullanƒ±lacak olan temel GAN eƒüitim mantƒ±ƒüƒ±nƒ± olu≈üturur.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11ZApEiw1gDG"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train_step(data, generator, discriminator, generator_optimizer, discriminator_optimizer):\n",
        "    images, _ = data\n",
        "    noise = tf.random.normal([tf.shape(images)[0], 100])\n",
        "\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "        generated_images = generator(noise, training=True)\n",
        "        real_output = discriminator(images, training=True)\n",
        "        fake_output = discriminator(generated_images, training=True)\n",
        "\n",
        "        gen_loss = generator_loss(fake_output)\n",
        "        dis_loss = discriminator_loss(real_output, fake_output)\n",
        "\n",
        "    generator_gradients = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    discriminator_gradients = disc_tape.gradient(dis_loss, discriminator.trainable_variables)\n",
        "\n",
        "    generator_optimizer.apply_gradients(zip(generator_gradients, generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(discriminator_gradients, discriminator.trainable_variables))\n",
        "\n",
        "    return gen_loss, dis_loss\n",
        "\n",
        "\n",
        "def train_gan(dataset, epochs, generator, discriminator, generator_optimizer, discriminator_optimizer, test_images, model_name, vis_interval=5):\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "\n",
        "        total_gen_loss = 0.0\n",
        "        total_dis_loss = 0.0\n",
        "        num_batches = 0\n",
        "\n",
        "        for image_batch in dataset:\n",
        "            gen_loss, dis_loss = train_step(image_batch, generator, discriminator, generator_optimizer, discriminator_optimizer)\n",
        "            total_gen_loss += gen_loss\n",
        "            total_dis_loss += dis_loss\n",
        "            num_batches += 1\n",
        "\n",
        "        avg_gen_loss = total_gen_loss / num_batches\n",
        "        avg_dis_loss = total_dis_loss / num_batches\n",
        "\n",
        "        print(f\"Generator Loss: {avg_gen_loss:.4f}, Discriminator Loss: {avg_dis_loss:.4f}\")\n",
        "\n",
        "        if (epoch + 1) % vis_interval == 0 or epoch == 0:\n",
        "            generate_and_plot_images(generator, test_images, title=f\"{model_name} - Epoch {epoch + 1}\")\n",
        "\n",
        "    print(f\"\\n‚úÖ Eƒüitim tamamlandƒ±: {model_name}\")\n",
        "    generate_and_plot_images(generator, test_images, title=f\"{model_name} - Final\")\n",
        "    generator.save(f\"{model_name}_generator.h5\")\n",
        "    discriminator.save(f\"{model_name}_discriminator.h5\")\n",
        "    print(f\"üíæ Kaydedildi: {model_name}_generator.h5, {model_name}_discriminator.h5\")\n",
        "\n",
        "\n",
        "def generate_and_plot_images(generator, test_images, title=\"GAN G√∂r√ºnt√ºleri\"):\n",
        "    generated_images = generator(test_images, training=False)\n",
        "    fig = plt.figure(figsize=(8, 8))\n",
        "    grid_size = int(np.sqrt(generated_images.shape[0]))\n",
        "\n",
        "    for i in range(generated_images.shape[0]):\n",
        "        plt.subplot(grid_size, grid_size, i + 1)\n",
        "        plt.imshow((generated_images[i].numpy() + 1) / 2)\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.suptitle(title, fontsize=16)\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMK2FQmU1gDG"
      },
      "source": [
        "## GAN Eƒüitiminin Ba≈ülatƒ±lmasƒ±\n",
        "\n",
        "### Sabit Test G√ºr√ºlt√ºs√º\n",
        "- Eƒüitim sƒ±rasƒ±nda g√∂rsellerin d√∂nemsel olarak √ºretilebilmesi i√ßin 16 adet sabit latent vekt√∂r (`test_images`) olu≈üturulmu≈ütur.\n",
        "\n",
        "\n",
        "###  `run_training()` Fonksiyonu\n",
        "- Belirtilen veri k√ºmesi i√ßin:\n",
        "  - Yeni bir **Generator** ve **Discriminator** olu≈üturulur.\n",
        "  - `AdamW` optimizasyon algoritmasƒ± ile her iki model eƒüitilir.\n",
        "  - Eƒüitim `train_gan()` fonksiyonu ile y√ºr√ºt√ºl√ºr.\n",
        "  - Klavye ile eƒüitim durdurulursa modeller ge√ßici olarak kaydedilir (`checkpoint`).\n",
        "\n",
        "---\n",
        "\n",
        "### √áoklu GAN Eƒüitimi\n",
        "- Her sƒ±nƒ±f (Normal, Benign, Malignant) i√ßin ayrƒ± GAN modelleri eƒüitilebilir.\n",
        "- ≈ûu anda yalnƒ±zca **Malignant** sƒ±nƒ±fƒ± i√ßin eƒüitim ba≈ülatƒ±lmƒ±≈ütƒ±r:\n",
        "\n",
        "```python\n",
        "run_training(train_dataset_malignant, \"gan_malignant\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "VbVomeLm1gDH",
        "outputId": "1a88c733-a204-4946-b19c-23364e5fd4cc"
      },
      "outputs": [],
      "source": [
        "# Sabit test g√ºr√ºlt√ºleri\n",
        "noise_dimensions = 100\n",
        "num_test_images = 16\n",
        "np.random.seed(123)\n",
        "tf.random.set_seed(123)\n",
        "test_images = tf.random.normal([num_test_images, noise_dimensions])\n",
        "\n",
        "# GPU i√ßin ayar (opsiyonel)\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
        "    except RuntimeError as e:\n",
        "        print(e)\n",
        "\n",
        "# Eƒüitim fonksiyonu √ßaƒürƒ±sƒ±\n",
        "def run_training(dataset, model_name):\n",
        "    generator = build_generator()\n",
        "    discriminator = build_discriminator()\n",
        "\n",
        "    generator_optimizer = tf.keras.optimizers.AdamW(learning_rate=0.00015, weight_decay=1e-4, beta_1=0.5, beta_2=0.999)\n",
        "    discriminator_optimizer = tf.keras.optimizers.AdamW(learning_rate=0.0001, weight_decay=1e-4, beta_1=0.5, beta_2=0.999)\n",
        "\n",
        "    try:\n",
        "        train_gan(\n",
        "            dataset=dataset,\n",
        "            epochs=3000,\n",
        "            generator=generator,\n",
        "            discriminator=discriminator,\n",
        "            generator_optimizer=generator_optimizer,\n",
        "            discriminator_optimizer=discriminator_optimizer,\n",
        "            test_images=test_images,\n",
        "            model_name=model_name\n",
        "        )\n",
        "    except KeyboardInterrupt:\n",
        "        print(f\"\\n Eƒüitim durduruldu: {model_name}\")\n",
        "        generator.save(f\"{model_name}_generator_interrupted.h5\")\n",
        "        discriminator.save(f\"{model_name}_discriminator_interrupted.h5\")\n",
        "        print(\"üíæ Checkpoint kaydedildi.\")\n",
        "\n",
        "\n",
        "# GAN eƒüitimini ba≈ülat\n",
        "run_training(train_dataset_normal, \"gan_normal\")\n",
        "run_training(train_dataset_benign, \"gan_benign\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#ƒ∞leri a≈üamalarda buraya gerek olmadƒ±ƒüƒ±nƒ± analiz ettim\n",
        "\n",
        "#run_training(train_dataset_malignant, \"gan_malignant\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5JPp30OCh8o"
      },
      "source": [
        "## GAN ile Sentetik G√∂r√ºnt√º √úretimi ve Kaydedilmesi (T√ºm Sƒ±nƒ±flar)\n",
        "\n",
        "Bu h√ºcre, daha √∂nce eƒüitilmi≈ü GAN modelleri (`gan_benign.h5`, `gan_malignant.h5`, `gan_normal.h5`) kullanƒ±larak **her bir sƒ±nƒ±f i√ßin 400 adet sentetik g√∂r√ºnt√º** √ºretmek amacƒ±yla √ßalƒ±≈ütƒ±rƒ±lƒ±r.\n",
        "\n",
        "### ƒ∞≈ülem Adƒ±mlarƒ±:\n",
        "1. **Model Y√ºkleme:** ƒ∞lgili sƒ±nƒ±fa ait GAN modeli `.h5` uzantƒ±sƒ±yla y√ºklenir.\n",
        "2. **Latent Boyutu:** Modelin giri≈ü ≈üekli (`latent_dim`) otomatik olarak algƒ±lanƒ±r.\n",
        "3. **Latent Vekt√∂r Olu≈üturma:** `z ‚àº N(0, 1)` daƒüƒ±lƒ±mƒ±ndan istenilen sayƒ±da √∂rnek √ºretilir.\n",
        "4. **G√∂r√ºnt√º √úretimi:** Generator model kullanƒ±larak sentetik g√∂r√ºnt√ºler elde edilir.\n",
        "5. **G√∂r√ºnt√º Kaydƒ±:**\n",
        "   - G√∂r√ºnt√ºler normalize edilip (0‚Äì255 aralƒ±ƒüƒ±na getirilip), `.png` olarak uygun klas√∂re kaydedilir.\n",
        "   - √ñrn: `generated_images_benign/`, `generated_images_malignant/`, `generated_images_normal/`\n",
        "\n",
        "> Bu i≈ülem her sƒ±nƒ±f i√ßin ayrƒ± ayrƒ± √ßalƒ±≈ütƒ±rƒ±lmalƒ±dƒ±r. √úretilen g√∂r√ºnt√ºler, CNN modeli i√ßin veri setini geni≈ületmede (augmentation) kullanƒ±lacaktƒ±r.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKLGRLSv2aP-",
        "outputId": "15aca5ce-513a-4f78-fcd3-1910777d1347"
      },
      "outputs": [],
      "source": [
        "generator = load_model(\"gan_benign_generator.h5\")\n",
        "\n",
        "\n",
        "input_shape = generator.input_shape\n",
        "latent_dim = input_shape[1]\n",
        "\n",
        "print(f\"[INFO] Detected latent dimension: {latent_dim}\")\n",
        "\n",
        "\n",
        "output_dir = \"generated_benign_normal\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "num_images = 1000\n",
        "latent_vectors = np.random.normal(0, 1, (num_images, latent_dim))\n",
        "\n",
        "\n",
        "generated_images = generator.predict(latent_vectors, verbose=1)\n",
        "\n",
        "\n",
        "for i, img in enumerate(generated_images):\n",
        "\n",
        "    img = np.clip(img, 0, 1)\n",
        "\n",
        "\n",
        "    img = (img * 255).astype(np.uint8)\n",
        "\n",
        "\n",
        "    if img.shape[-1] == 1:\n",
        "        img = img.squeeze(-1)\n",
        "\n",
        "\n",
        "    plt.imsave(f\"{output_dir}/image_{i+1:03d}.png\", img, cmap='gray' if len(img.shape) == 2 else None)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEYqaJpXEHFj"
      },
      "source": [
        "## Ger√ßek ve GAN G√∂r√ºnt√ºlerinin Kar≈üƒ±la≈ütƒ±rmalƒ± G√∂rselle≈ütirmesi\n",
        "### G√∂rselle≈ütirilen Sƒ±nƒ±flar:\n",
        "- Normal cases\n",
        "- Benign cases\n",
        "\n",
        "### Adƒ±mlar:\n",
        "1. Her sƒ±nƒ±f i√ßin:\n",
        "   - `num_samples` kadar **ger√ßek** ve **sentetik** g√∂r√ºnt√º rastgele se√ßilir.\n",
        "2. G√∂rseller bir subplot grid i√ßerisinde yan yana yerle≈ütirilir:\n",
        "   - Sol s√ºtunlar: Ger√ßek g√∂r√ºnt√ºler.\n",
        "   - Saƒü s√ºtunlar: GAN tarafƒ±ndan √ºretilmi≈ü sentetik g√∂r√ºnt√ºler.\n",
        "\n",
        "> Bu kar≈üƒ±la≈ütƒ±rma, GAN modellerinin g√∂rsel kalitesini deƒüerlendirmek ve √ºretimlerinin ger√ßek g√∂r√ºnt√ºlere ne kadar yakƒ±n olduƒüunu g√∂zlemlemek i√ßin √∂nemlidir.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "qGIs--pX3oQu",
        "outputId": "105f4aa6-eb57-4854-9446-3a2122dfed07"
      },
      "outputs": [],
      "source": [
        "classes = [\"Normal cases\", \"Bengin cases\"]\n",
        "num_samples = 5\n",
        "\n",
        "real_base_path = \"/content/lung_dataset/The IQ-OTHNCCD lung cancer dataset\"\n",
        "gan_base_paths = {\n",
        "    \"Normal cases\": \"generated_images_normal\",\n",
        "    \"Bengin cases\": \"generated_images_benign\"\n",
        "}\n",
        "\n",
        "fig, axs = plt.subplots(len(classes), num_samples * 2, figsize=(num_samples * 2.5, len(classes) * 3))\n",
        "\n",
        "for row_idx, class_name in enumerate(classes):\n",
        "\n",
        "    real_path = os.path.join(real_base_path, class_name)\n",
        "    gan_path = gan_base_paths[class_name]\n",
        "\n",
        "    real_images = random.sample(os.listdir(real_path), num_samples)\n",
        "    gan_images = random.sample(os.listdir(gan_path), num_samples)\n",
        "\n",
        "    for i in range(num_samples):\n",
        "\n",
        "        real_img = Image.open(os.path.join(real_path, real_images[i]))\n",
        "        axs[row_idx, i].imshow(real_img, cmap='gray' if real_img.mode == 'L' else None)\n",
        "        axs[row_idx, i].axis('off')\n",
        "        axs[row_idx, i].set_title(f\"{class_name} - Real\", fontsize=8)\n",
        "\n",
        "\n",
        "        gan_img = Image.open(os.path.join(gan_path, gan_images[i]))\n",
        "        axs[row_idx, num_samples + i].imshow(gan_img, cmap='gray' if gan_img.mode == 'L' else None)\n",
        "        axs[row_idx, num_samples + i].axis('off')\n",
        "        axs[row_idx, num_samples + i].set_title(f\"{class_name} - GAN\", fontsize=8)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python (tensorflow_env)",
      "language": "python",
      "name": "tensorflow_env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
